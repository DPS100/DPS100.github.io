[
    {
        "title": "Portfolio Site",
        "goal": "Create and self-host a portfolio site.",
        "description": "A custom and easily configurable portfolio built with Javascript, HTML, and CSS. I can easily incorporate a new project simply by updating a few lines in the projects.json manifest!",
        "role":"Creator",
        "metadata": {
          "date": "2024-01-15",
          "images": [

          ],
          "videos": [

          ],
          "media": [

          ],
          "links": [
            {"title": "Github", "link": "https://github.com/DPS100/DPS100.github.io"}
          ],
          "skills": [
            "Javascript",
            "HTML",
            "CSS",
            "Front-End Development"
          ],
          "categories": [
            {"filter": "software", "relevance": 0.5}
          ]
        }
    },
    {
        "title": "Whirly Tunes",
        "goal": "Explore pitch possibilities, types of form and development, and capabilities unique to musical machines.",
        "description": "A robotic ensemble of colorful modules that produce a voice-like musical tone. I contributed to the research, prototyping, assembly, DAW communication, and creating a musical score to interface with the robot. This was my humanities final practicum in Musical Robotics, where students were tasked with creating musical robots and complimentary scores.",
        "role":"Contributor",
        "metadata": {
          "date": "2024-01-15",
          "images": [
            "assets/images/whirly-tunes.jpg",
            "assets/images/whirly_hub.png"
          ],
          "videos": [
            "https://wp.wpi.edu/musicalmachines/files/2023/12/PXL_20231215_192046095.TS_.mp4"
          ],
          "media": [
            "assets/media/Whirly-tunes.pdf"
          ],
          "links": [
            {"title": "Read more", "link": "https://wp.wpi.edu/musicalmachines/2023/12/17/whirly-tunes/"},
            {"title": "Github", "link": "https://github.com/Kreeevin/WhirlyBoi"}
          ],
          "skills": [
            "Python",
            "CAD",
            "Embedded Systems"
          ],
          "categories": [
            {"filter": "robotics", "relevance": 0.2}
          ]
        }
    },
    {
        "title": "Stop Motion Armature",
        "goal": "Create an expressive figure for a stop motion film.",
        "description": "Short film project for my high school Film as Literature class, where I created a stop motion movie. I designed, iterated, and 3D printed a custom armature from scratch, prototyping different kinds of actuating joints, hands, and facial expressions in a CAD program (Onshape). I also created a basic interface with OpenCV to quickly capture frames and play videos taken from these frames to preview the armature's movement.",
        "role":"Creator",
        "metadata": {
          "date": "2024-01-15",
          "images": [

          ],
          "videos": [
            "https://www.youtube.com/embed/1XkwiTnlhTY"
          ],
          "media": [

          ],
          "links": [

          ],
          "skills": [
            "CAD",
            "3D printing"
          ],
          "categories": [
            {"filter": "robotics", "relevance": 0.1}
          ]
        }
    },
    {
        "title": "Probabalistic Imitation Learning",
        "goal": "Probabilistic Modeling of Human Actions for RTS Games with Limited Reinforcement Learning Feasibility: A Clash Royale™ Case Study.",
        "description": "Final project for my graduate Deep Learning class. In this paper, we explore using human replays as a means to train a deep learning model to play the popular real-time strategy game Clash Royale™. This work is inspired by previous reinforcement learning approaches, which we believe are not scalable for Clash Royale™, due to its extremely large card selection, number of possible deck combinations, lack of high-speed API, and difficult-to-identify rewards. We find that our imitation approach is functional but limited in performance when trained on a small dataset, but shows promise if it could be scaled up. We present our findings and takeaways from the entire process, from data collection and label extraction to training and deploying a model to play against real players.",
        "role":"Co-Author",
        "metadata": {
          "date": "2025-12-10",
          "images": ["assets/images/DLPpic.png"],
          "videos": [],
          "media": ["assets/media/CS541FinalProject__Copy_.pdf"],
          "links": [
            {"title": "Github", "link": "https://github.com/chrisrca/CS541-Deep-Learning-Clash-Royale-Project/tree/cards-from-image"},
            {"title": "HuggingFace", "link": "https://huggingface.co/datasets/chrisrca/clash-royale-tv-replays"}
          ],
          "skills": [
            "Python",
            "PyTorch",
            "Computer Vision",
            "Machine Learning",
            "Deep Learning",
            "Data Pipelining",
            "Data Cleaning",
            "Image Processing"
          ],
          "categories": [
            {"filter": "data-science-ml", "relevance": 0.9},
            {"filter": "software", "relevance": 0.4}
          ]
        }
    },
    {
        "title": "Autonomous Maze Navigation",
        "goal": "Program a robot to autonomously explore and navigate in an unknown environment.",
        "description": "The cumulative final project for my Unified Robotics IV: Navigation class to showcase our understanding of robot self localization and mapping (SLAM). The final demo consisted of three parts: Mapping out an unknown maze, returning to the start position, and returning to the start position once again after being relocated to some random position in the maze at a random time. Gazebo and RViz were used to simulate and test code before deployment. The robot itself is a turtlebot 3 burger running ROS, mapping it's immediate surroundings using a LIDAR sensor. It would report readings back to a host computer over ROS topics, which would estimate the position of the robot to the maze walls using a Kalman filter according to the internal wheel odometry and external wall positions. OpenCV was used to compute unexplored frontiers to investigate. An A* algorithm with a heuristic was used to determine the best frontier and how to navigate to it, and a cubic spline was provided for the turtlebot to follow. When the turtlebot detected it had been disturbed with an accelerometer, it used adaptive Monte Carlo localization to find where it was in the maze, and return to the starting position.",
        "role":"Contributor",
        "metadata": {
          "date": "2024-01-15",
          "images": [
            "assets/images/3002CSpace.png",
            "assets/images/3002Deploy.png",
            "assets/images/3002Map.png"
          ],
          "videos": [],
          "media": ["assets/media/Lab4_RBE3002_Report (1).pdf"],
          "links": [
          ],
          "skills": [
            "Teamwork",
            "Python",
            "ROS",
            "Linux",
            "Robot Navigation",
            "Trajectory Planning",
            "Image Processing",
            "Distributed and Edge Systems",
            "Event-Driven Systems"
          ],
          "categories": [
            {"filter": "robotics", "relevance": 0.86},
            {"filter": "software", "relevance": 0.45}
          ]
        }
    },
    {
        "title": "Robotic Pick and Place",
        "goal": "Program a robotic arm to identify and manipulate different types of objects inside the workspace.",
        "description": "The cumulative final project for my Unified Robotics III: Manipulation class. The demo entailed detecting objects of interest, determining their position in the workspace, and controlling the robotic arm to sort them into their respective bins. Both the controls for the OpenManipulator-X arm and vision processing pipeline were built using MATLAB. The arm consisted of four servos that could control position and velocity. The dimensions of the arm linkages and position of the servos could be used to derive the position of the arm in space with forward kinematics. Once the position of an object of interest was identified, inverse kinematics was to determine the configuration of the arm to manipulate the target. In order to reach the target, we calculated the Jacobian of the end effector, which could be used to translate between the robot joint space and the working space. Using cubic trajectory interpolation and the Jacobian, the robot can generate a smooth control signal to manipulate the target objects.",
        "role":"Contributor",
        "metadata": {
          "date": "2024-01-15",
          "images": ["assets/images/OMX.png", "assets/images/Balls.png"],
          "videos": ["https://www.youtube.com/embed/Q2C-VMDZcdc"],
          "media": ["assets/media/Lab5_Report_RBE3001.pdf"],
          "links": [],
          "skills": [
            "Teamwork",
            "MATLAB",
            "Control Theory",
            "Image Processing",
            "Robot Kinematics",
            "Motion Planning",
            "Embedded Systems"
          ],
          "categories": [
            {"filter": "robotics", "relevance": 0.85},
            {"filter": "software", "relevance": 0.4}
          ]
        }
    },
    {
        "title": "FRC: Jazzy Judi",
        "goal": "Create a competitive robot for the 2022 First Robotics Competetion by implementing and automating robotic subsystems.",
        "description": "Jazzy Judi was the robot built by my high school robotics team for the 2022 FIRST Robotics Competition (FRC) game Rapid React. My duties to the team included distributing work to other programmers, teaching new members, and communicating with other sub-teams what was in progress or needed to be done. I was also the operator during competitions, diagnosing problems and modifying the code on the fly according to the demands of the current match. The technical solutions I implemented for the robot ranged from game piece tracking using OpenCV and GRIP, automation of subsystems to keep the driver focused on the game, and creation of one of the best autonomous routines in the state. </p><p>I implemented three main sections of automation: Driving, game piece manipulation, and climbing. For driving, I used internal odometry and a trajectory tracking algorithms to follow splines supplied to the robot. Additionally, I implemented a vision pipeline with GRIP and OpenCV to track and follow game pieces while ignoring similar objects. Game piece manipulation consisted of coordinating internal subsystems including the intake, indexer, and shooter to make scoring as simple as pressing a button to intake a ball, and another button to score. The first button press would deploy the protected intake from behind the bumper to grab a ball. The ball would move into the indexer, where a color sensor would determine if it was our team's ball, ejecting the ball if it was not our own. The shooter flywheels would then be spun up to the correct speed to score from where we were on the field, and the driver would press the next button to score. Finally, the climbing sequence  was carfully calibrated to move as quickly and safely up the rungs as possible by managaging velocity and external torque on the arms.</p><p>Some background on the competition to help put my contributions into context: the 2022 Rapid React First Robotics Competition consisted of two alliances of three robots from individual teams, each trying to score more points than the other. Points were scored by shooting game pieces, essentially basketball sized tennis balls, into a funnel at the center of the field. The task I am the most proud of is the four ball autonomous routine, where the robot collected and scored four game pieces in the first 15 seconds of the game. This was a huge factor in our success winning the Colorado regional and ranking 7th in our division of the international championship.",
        "role":"Lead Programmer",
        "metadata": {
          "date": "2024-01-15",
          "images": [
            "assets/images/judy.jfif"
          ],
          "videos": [
            "https://www.youtube.com/embed/RvorwGZfUQQ",
            "https://www.youtube.com/embed/l-FUae_8Po4"
          ],
          "media": [

          ],
          "links": [
            {"title": "Angelbotics Website", "link": "https://angelbotics.org"},
            {"title": "Github", "link": "https://github.com/Angelbots1339/2022_Competition_Season"}
          ],
          "skills": [
            "Leadership",
            "Teamwork",
            "Java",
            "Event-Driven Systems",
            "Embedded Systems",
            "Image Processing",
            "Robot Kinematics",
            "Robot Navigation",
            "Control Theory",
            "Motion Planning",
            "Trajectory Planning"
          ],
          "categories": [
            {"filter": "robotics", "relevance": 0.9},
            {"filter": "software", "relevance": 0.3}
          ]
        }
    },
    {
        "title": "Robotic Surgical Laser",
        "goal": "Lasers are widely used in surgery to perform precise tissue cutting and incisions. The goal of this research is to create an application for an iPad (or other tablet) that allows precise control of a surgical laser using a stylus, such as the Apple Pencil. The idea is to enable surgeons to control the laser as if they were writing with it. The overarching objective of this research is to make laser aiming more intuitive, precise, and less prone to errors. This research is ongoing in the Comet Lab at Gateway and will be completed May 2026.",
        "description": "I'm currently working under Loris Fichera at WPI's COMET Lab, with my current research creating an interface for a 7-DOF Franka Emika robotic arm with a laser end effector for use by a touchscreen device. I own system architecture and deployment, implementing control and data transfer over WebSockets between microservices and edge devices for low-latency teleoperation. Additionally, I am designing control algorithms to respect active constraints/virtual fixtures alongside computer vision algorithms to stabilize the end effector relative to a dynamic work surface. We have conducted stakeholder interviews and produced a fully functional MVP, with ongoing user studies to conclude this project in May of 2026.",
        "role":"Undergraduate Research",
        "metadata": {
          "date": "2025-08-01",
          "images": [
            "assets/images/laser-robot.png",
            "assets/images/laser-block.jpeg"
          ],
          "videos": [
            "assets/media/laser_mobile_control.mp4",
            "assets/media/laser-surgery.mp4",
            "assets/media/laser_transformed_view_path.mp4"
          ],
          "media": [
            "assets/media/Scoping_Doc.pdf"
          ],
          "links": [
            {"title": "Github", "link": "https://github.com/comet-lab/Laser-Stylus-MQP"}
          ],
          "skills": [
            "Teamwork",
            "Python",
            "Real-Time Systems",
            "Back-End Development",
            "Image Processing",
            "Computer Vision",
            "Robot Kinematics",
            "Trajectory Planning",
            "Linux",
            "System Architecture"
          ],
          "categories": [
            {"filter": "robotics", "relevance": 1.0},
            {"filter": "software", "relevance": 0.51}
          ]
        }
    },
    {
      "title": "Smarter Learning Environments",
      "goal": "Environmental conditions in classrooms play a critical role in shaping students' health, cognitive function, and academic performance. A novel environmental data collection/analysis system was developed and deployed to take detailed samples and provide informative visuals of temperature, humidity, air quality, noise, and lighting. Perceptions of environmental factors were evaluated through outreach surveys and one-on-one interviews. Recommendations were proposed to the Universidad de Cádiz to help address deficiencies found in ambient classroom conditions and promote student well-being and learning.",
      "description": "I spent a semester abroad in Spain working with researchers at the Universidad de Cadiz building embedded IoT modules and a full stack dashboard to monitor indoor air quality. I was responsible for system design and deployment (both hardware and software) and programming embedded modules. Through our research and data-driven discoveries, we found a high concentration of particulate matter in labs with inadequate ventilation.",
      "role":"Co-Author",
      "metadata": {
        "date": "2025-05-06",
        "images": [
          "assets/images/SLE_Devices.jpg",
          "assets/images/SLE_Dashboard.png"
        ],
        "videos": [

        ],
        "media": [
          "https://docs.google.com/presentation/d/e/2PACX-1vQcWRVMJf9cwEN3cMSkvKCaZGpO1m0SBrp7AcAy-_Eqrb9RgvV8Eryj-5GyXhjDgA/pubembed?start=false&loop=true&delayms=3000",
          "assets/media/Smarter Learning Environments Final Report.pdf"
        ],
        "links": [
          {"title": "Github", "link": "https://github.com/Smarter-Learning-Environments"},
          {"title": "Paper", "link": "https://digital.wpi.edu/concern/student_works/fx719r37x"}
        ],
        "skills": [
          "Leadership",
          "Teamwork",
          "C++",
          "Embedded Systems",
          "Back-End Development",
          "Distributed and Edge Systems",
          "CAD",
          "Python",
          "Linux",
          "Data Pipelining"
        ],
        "categories": [
            {"filter": "robotics", "relevance": 0.4},
            {"filter": "software", "relevance": 0.8},
            {"filter": "data-science-ml", "relevance": 0.2}
        ]
      }
    }
]